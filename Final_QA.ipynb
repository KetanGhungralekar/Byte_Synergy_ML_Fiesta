{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2kXxl9fA7iz",
    "outputId": "02ca675a-6717-46e1-acd7-e1e045669293"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.6.0)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.5)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
      "Requirement already satisfied: gradio-client==1.4.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.3)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.4)\n",
      "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.1)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.2)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.3->gradio) (2024.10.0)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.3->gradio) (12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (2.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.10/dist-packages (20240930)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.26.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.8.0)\n",
      "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (2.1.5)\n",
      "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.46.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.26.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio\n",
    "!pip install -U openai-whisper\n",
    "!pip install sentence_transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVITyyLf1btl"
   },
   "source": [
    "# QA Model's for answering questions given as input in the form of audio\n",
    "## Whisper was used to transcribe the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bzqqvZyMBhXC",
    "outputId": "e4026c30-37c2-4012-9c80-221925555947"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n",
      "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import whisper\n",
    "# import librosa\n",
    "# from transformers import pipeline\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# # Set the device to GPU if available\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Load Whisper ASR model on GPU\n",
    "# model_m = whisper.load_model(\"medium\", device=device)\n",
    "\n",
    "# # Load Hugging Face Transformers pipeline for question answering\n",
    "# qa_pipeline = pipeline('question-answering', model=\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\", device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "# # Load Sentence Transformer model for semantic search\n",
    "# semantic_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "# def transcribe_question(input_question_file):\n",
    "#     # Load and resample audio to 16kHz\n",
    "#     question_audio, sr = librosa.load(input_question_file, sr=16000)\n",
    "#     whisper_audio = torch.tensor(question_audio, dtype=torch.float32).to(device)\n",
    "#     # Perform transcription on the question audio\n",
    "#     result = model_m.transcribe(whisper_audio, language=\"en\", fp16=torch.cuda.is_available())\n",
    "#     return result[\"text\"]\n",
    "\n",
    "# def generate_answer(question_text, context):\n",
    "#     # Use the Hugging Face Transformers pipeline to extract the answer\n",
    "#     answer = qa_pipeline({\n",
    "#         \"question\": question_text,\n",
    "#         \"context\": context\n",
    "#     })\n",
    "#     return answer[\"answer\"]\n",
    "\n",
    "# def semantic_search(query, segments):\n",
    "#     # Embed the query and segments to compute cosine similarity\n",
    "#     query_embedding = semantic_model.encode(query, convert_to_tensor=True)\n",
    "#     segment_embeddings = semantic_model.encode(segments, convert_to_tensor=True)\n",
    "\n",
    "#     # Compute cosine similarities\n",
    "#     similarities = util.pytorch_cos_sim(query_embedding, segment_embeddings)[0]\n",
    "\n",
    "#     # Get the index of the most similar segment\n",
    "#     most_similar_idx = torch.argmax(similarities).item()\n",
    "#     return segments[most_similar_idx]\n",
    "\n",
    "# def speech_based_qa_pipeline(input_question_file, transcription_files):\n",
    "#     # Step 1: Transcribe the question\n",
    "#     question_text = transcribe_question(input_question_file)\n",
    "#     print(\"User Question:\", question_text)\n",
    "\n",
    "#     # Step 2: Load and concatenate transcriptions from files as separate segments\n",
    "#     segments = []\n",
    "#     for file_path in transcription_files:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             segments.append(f.read().strip())  # Add each file's transcription as a separate segment\n",
    "\n",
    "#     # Step 3: Perform semantic search to find the most relevant segment\n",
    "#     relevant_segment = semantic_search(question_text, segments)\n",
    "\n",
    "#     # Step 4: Generate answer using the Hugging Face Transformers pipeline with the relevant context\n",
    "#     final_answer = generate_answer(question_text, relevant_segment)\n",
    "#     print(\"Final Answer:\", final_answer)\n",
    "\n",
    "#     return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "G_qOa1LlxRym",
    "outputId": "18fdb574-5145-4d1b-e272-7e818f1d106b"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7c8aa91a198d>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load Whisper ASR model on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwhisper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"medium\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load Hugging Face Transformers pipeline for question answering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, device, download_root, in_memory)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MODELS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mcheckpoint_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MODELS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0malignment_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ALIGNMENT_HEADS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/whisper/__init__.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(url, root, in_memory)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mmodel_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_sha256\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_bytes\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0min_memory\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdownload_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import torch\n",
    "# import whisper\n",
    "# import librosa\n",
    "# from transformers import pipeline\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# # Set the device to GPU if available\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # Load Whisper ASR model on GPU\n",
    "# model_m = whisper.load_model(\"medium\", device=device)\n",
    "\n",
    "# # Load Hugging Face Transformers pipeline for question answering\n",
    "# qa_pipeline = pipeline('question-answering', model=\"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\", device=0 if device == \"cuda\" else -1)\n",
    "\n",
    "# # Load Sentence Transformer model for semantic search\n",
    "# semantic_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "# # Path to feedback log file\n",
    "# feedback_log_path = \"feedback_log.json\"\n",
    "\n",
    "# # Load feedback log if it exists\n",
    "# if os.path.exists(feedback_log_path):\n",
    "#     with open(feedback_log_path, 'r', encoding='utf-8') as f:\n",
    "#         feedback_log = json.load(f)\n",
    "# else:\n",
    "#     feedback_log = {}\n",
    "\n",
    "# def transcribe_question(input_question_file):\n",
    "#     # Load and resample audio to 16kHz\n",
    "#     question_audio, sr = librosa.load(input_question_file, sr=16000)\n",
    "#     whisper_audio = torch.tensor(question_audio, dtype=torch.float32).to(device)\n",
    "#     # Perform transcription on the question audio\n",
    "#     result = model_m.transcribe(whisper_audio, language=\"en\", fp16=torch.cuda.is_available())\n",
    "#     return result[\"text\"]\n",
    "\n",
    "# def generate_answer(question_text, context):\n",
    "#     # Use the Hugging Face Transformers pipeline to extract the answer\n",
    "#     answer = qa_pipeline({\n",
    "#         \"question\": question_text,\n",
    "#         \"context\": context\n",
    "#     })\n",
    "#     return answer[\"answer\"]\n",
    "\n",
    "# def semantic_search(query, segments):\n",
    "#     # Embed the query and segments to compute cosine similarity\n",
    "#     query_embedding = semantic_model.encode(query, convert_to_tensor=True)\n",
    "#     segment_embeddings = semantic_model.encode(segments, convert_to_tensor=True)\n",
    "\n",
    "#     # Compute cosine similarities\n",
    "#     similarities = util.pytorch_cos_sim(query_embedding, segment_embeddings)[0]\n",
    "\n",
    "#     # Get the index of the most similar segment\n",
    "#     most_similar_idx = torch.argmax(similarities).item()\n",
    "#     return segments[most_similar_idx]\n",
    "\n",
    "# def save_feedback(question_text, correct_answer):\n",
    "#     # Store corrected answer in feedback log and save to file\n",
    "#     feedback_log[question_text] = correct_answer\n",
    "#     with open(feedback_log_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(feedback_log, f, ensure_ascii=False, indent=4)\n",
    "#     print(f\"Feedback saved for question '{question_text}'.\")\n",
    "\n",
    "# def check_feedback(question_text):\n",
    "#     # Check if feedback exists for the question\n",
    "#     return feedback_log.get(question_text, None)\n",
    "\n",
    "# def speech_based_qa_pipeline(input_question_file, transcription_files):\n",
    "#     # Step 1: Transcribe the question\n",
    "#     question_text = transcribe_question(input_question_file)\n",
    "#     print(\"User Question:\", question_text)\n",
    "\n",
    "#     # Check if feedback is available for a similar question\n",
    "#     corrected_answer = check_feedback(question_text)\n",
    "#     if corrected_answer:\n",
    "#         print(\"Corrected Answer from Feedback Log:\", corrected_answer)\n",
    "#         return corrected_answer  # Return the feedback if available\n",
    "\n",
    "#     # Step 2: Load and concatenate transcriptions from files as separate segments\n",
    "#     segments = []\n",
    "#     for file_path in transcription_files:\n",
    "#         with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#             segments.append(f.read().strip())  # Add each file's transcription as a separate segment\n",
    "\n",
    "#     # Step 3: Perform semantic search to find the most relevant segment\n",
    "#     relevant_segment = semantic_search(question_text, segments)\n",
    "\n",
    "#     # Step 4: Generate answer using the Hugging Face Transformers pipeline with the relevant context\n",
    "#     final_answer = generate_answer(question_text, relevant_segment)\n",
    "#     print(\"Generated Answer:\", final_answer)\n",
    "\n",
    "#     # Step 5: Request feedback from user\n",
    "#     user_feedback = input(\"Is the answer correct? (yes/no): \").strip().lower()\n",
    "#     if user_feedback == \"no\":\n",
    "#         corrected_answer = input(\"Please provide the correct answer: \").strip()\n",
    "#         save_feedback(question_text, corrected_answer)\n",
    "#         return corrected_answer\n",
    "#     else:\n",
    "#         print(\"Answer confirmed as correct.\")\n",
    "#         return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hhIADLrN1btm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "import librosa\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Set the device to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set up GenAI API key\n",
    "os.environ[\"API_KEY\"] = \"AIzaSyBZTAdmQ-cAi0wKD_z9hvSt_r5qjgm13lY\"  # Replace with your actual GenAI key\n",
    "genai.configure(api_key=os.environ[\"API_KEY\"])\n",
    "\n",
    "# Load Whisper ASR model on GPU\n",
    "model_m = whisper.load_model(\"medium\", device=device)\n",
    "\n",
    "# Load Sentence Transformer model for semantic search\n",
    "semantic_model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "\n",
    "\n",
    "def transcribe_question(input_question_file):\n",
    "    \"\"\"\n",
    "    Transcribe audio input containing the question using Whisper.\n",
    "    \"\"\"\n",
    "    # Load and resample audio to 16kHz\n",
    "    question_audio, sr = librosa.load(input_question_file, sr=16000)\n",
    "    whisper_audio = torch.tensor(question_audio, dtype=torch.float32).to(device)\n",
    "    # Perform transcription on the question audio\n",
    "    result = model_m.transcribe(whisper_audio, language=\"en\", fp16=torch.cuda.is_available())\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "def generate_answer_with_genai(question_text, context):\n",
    "    \"\"\"\n",
    "    Use GenAI to generate an answer to the question based on the provided context.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Prepare the prompt for GenAI\n",
    "        prompt = (\n",
    "                  f\"Answer the following question based on the provided context:\\n\\n\"\n",
    "                  f\"Question: {question_text}\\n\"\n",
    "                  f\"Context: {context}\\n\"\n",
    "                  f\"Answer: \"\n",
    "                  f\"Provide only the answer without any explanation. If the question contains spelling or grammar mistakes, correct them and provide the best possible answer. If the answer is not directly available, give the most relevant or related information. Do not say 'this passage does not contain an answer.' Just provide a close, relevant answer.\"\n",
    "          )\n",
    "\n",
    "        # Use GenAI to generate the answer\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")  # Specify the model to use\n",
    "        response = model.generate_content(prompt)\n",
    "\n",
    "        # Extract the generated answer from the response\n",
    "        answer = response.text\n",
    "        return answer.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer with GenAI: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def semantic_search(query, segments):\n",
    "    \"\"\"\n",
    "    Perform semantic search to find the most relevant context segment for the query.\n",
    "    \"\"\"\n",
    "    # Embed the query and segments to compute cosine similarity\n",
    "    query_embedding = semantic_model.encode(query, convert_to_tensor=True)\n",
    "    segment_embeddings = semantic_model.encode(segments, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarities\n",
    "    similarities = util.pytorch_cos_sim(query_embedding, segment_embeddings)[0]\n",
    "\n",
    "    # Get the index of the most similar segment\n",
    "    most_similar_idx = torch.argmax(similarities).item()\n",
    "    return segments[most_similar_idx]\n",
    "\n",
    "\n",
    "def speech_based_qa_pipeline(input_question_file, transcription_files):\n",
    "    \"\"\"\n",
    "    Perform the full speech-based QA pipeline:\n",
    "    - Transcribe the question from audio.\n",
    "    - Perform semantic search on context segments.\n",
    "    - Use GenAI for QA.\n",
    "    \"\"\"\n",
    "    # Step 1: Transcribe the question\n",
    "    question_text = transcribe_question(input_question_file)\n",
    "    print(\"User Question:\", question_text)\n",
    "\n",
    "    # Step 2: Load and concatenate transcriptions from files as separate segments\n",
    "    segments = []\n",
    "    for file_path in transcription_files:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            segments.append(f.read().strip())  # Add each file's transcription as a separate segment\n",
    "\n",
    "    # Step 3: Perform semantic search to find the most relevant segment\n",
    "    relevant_segment = semantic_search(question_text, segments)\n",
    "\n",
    "    # Step 4: Generate answer using GenAI with the relevant context\n",
    "    final_answer = generate_answer_with_genai(question_text, relevant_segment)\n",
    "    print(\"Final Answer:\", final_answer)\n",
    "\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "id": "OeA2LJoMBsDX",
    "outputId": "70abef64-7f8c-4f9b-ebbb-a57f955bc1fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://712d3f0b371f2126bb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://712d3f0b371f2126bb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file path: /tmp/gradio/0ab9b75b62880651fd7ccaddcfff04c24d2cf8fe67fc938c28a0d168ff24514c/SandalWoodNewsStories_107.mp3\n",
      "User Question:  Now we are going to see the Kaulai Gada. Why do we need to climb Kaulai Gada? We call it Kaulaiyaan in Kannada. In English we call it Koronda. Here we have beautiful fruits. But between the beautiful flowers there is a thorn. What this thorn does is it is used as a fence. If someone is cleaning the fence it is used as a natural fence. This is a hybrid. It is 70 rupees per kg. It is a profit. If you have a bird or a bird's nest you can use it as a fence. But look at the aesthetics. If you want this body and want to know more about the number subscribe to our channel. This is a single tree. This is a fence. This is a fruit tree. It is a profit.\n",
      "Final Answer: To utilize its thorns as a natural fence and harvest its beautiful fruits.\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://712d3f0b371f2126bb.gradio.live\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import tempfile\n",
    "from shutil import copyfile\n",
    "\n",
    "# Dummy transcription file for illustration purposes\n",
    "transcription_files = [\n",
    "    r\"./final_combined_output.txt\"\n",
    "]\n",
    "\n",
    "def process_audio_question(audio_file):\n",
    "    \"\"\"\n",
    "    Processes the audio file and provides an answer.\n",
    "    Ensures playback is displayed only after the file is fully ready.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Debugging: Check the audio file's path\n",
    "        print(f\"Audio file path: {audio_file}\")\n",
    "\n",
    "        # Copy the uploaded/recorded audio to a temporary file for playback\n",
    "        temp_audio_path = tempfile.mktemp(suffix=\".mp3\")  # Adjust suffix based on audio format\n",
    "        copyfile(audio_file, temp_audio_path)\n",
    "\n",
    "        # Simulate the question-answering process\n",
    "        answer = speech_based_qa_pipeline(audio_file, transcription_files)\n",
    "\n",
    "        # Return the processed audio path and the answer\n",
    "        return temp_audio_path, answer\n",
    "    except Exception as e:\n",
    "        print(f\"Debug: Error in process_audio_question: {str(e)}\")\n",
    "        return None, f\"An error occurred during processing: {str(e)}\"\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as iface:\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\n",
    "            \"\"\"\n",
    "            # üéôÔ∏è Sandalwood Stories QA System\n",
    "            Ask your question about Sandalwood by recording or uploading an audio file.\n",
    "            The system will process your question and provide an answer.\n",
    "            \"\"\",\n",
    "            elem_id=\"main-header\"\n",
    "        )\n",
    "\n",
    "    with gr.Row():\n",
    "        # Audio input field and answer display on the same horizontal line\n",
    "        audio_input = gr.Audio(type=\"filepath\", label=\"üéß Record or Upload Audio Question\", interactive=True)\n",
    "        output_text = gr.Textbox(label=\"üìù Answer\", interactive=False, lines=10)  # Set height by increasing `lines`\n",
    "\n",
    "    # Trigger processing when the audio changes\n",
    "    def process_and_display(audio_file):\n",
    "        # Directly call the audio processing function\n",
    "        processed_audio, answer = process_audio_question(audio_file)\n",
    "        return answer\n",
    "\n",
    "    audio_input.change(\n",
    "        process_and_display,\n",
    "        inputs=audio_input,\n",
    "        outputs=output_text  # Update the answer display\n",
    "    )\n",
    "    \n",
    "iface.launch(debug=True, share=True)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
